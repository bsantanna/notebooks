{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-15T12:12:37.436385Z",
     "start_time": "2023-08-15T12:12:37.432694Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade langchain llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T12:12:45.901756Z",
     "start_time": "2023-08-15T12:12:37.439821Z"
    }
   },
   "id": "2504372d42308e93"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Setup on MacBook Pro M1\n",
    "\n",
    "1. [Requested access](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n",
    "2. Downloaded model from [HF model from Hugging Face](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)\n",
    "3. Cloned [llama.cpp](https://github.com/ggerganov/llama.cpp.git)\n",
    "4. From llamma.cpp root directory, converted model to ggml format with `python convert.py <path_to_downloaded>/Llama-2-7b-chat-hf`\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27e6bae5bfc7c353"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/bsantanna/dev/workspace/community/Llama-2-7b-chat-hf/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 5504\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 13155.10 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/bsantanna/miniforge3/envs/ml/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x13ffc64a0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x13ffcc820\n",
      "ggml_metal_init: loaded kernel_mul                            0x13ffccba0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x13ffcce00\n",
      "ggml_metal_init: loaded kernel_scale                          0x13ffcd060\n",
      "ggml_metal_init: loaded kernel_silu                           0x13ffcd2c0\n",
      "ggml_metal_init: loaded kernel_relu                           0x13ffcd520\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13ffcd780\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13ffcd9e0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x13ffcdc40\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13ffcdea0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13ffce100\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13ffce360\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x13ffce5c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x13ffce820\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x13ffcea80\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x13ffcece0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x13ffcef40\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x13ffcf1a0\n",
      "ggml_metal_init: loaded kernel_norm                           0x13ffcf400\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x13ffcf9a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x13ffcfc00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x13ffcfe60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x13ffd00c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x13ffd0320\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x13ffd0580\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x13ffd07e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x13ffd0a40\n",
      "ggml_metal_init: loaded kernel_rope                           0x13ffd0ca0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x13ffd12a0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x13ffd1870\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x13ffd1e40\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x13ffd2410\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 49152.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =   250.00 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.45 MB, (12853.91 / 49152.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.00 MB, (12863.91 / 49152.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB, (13121.91 / 49152.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   132.00 MB, (13253.91 / 49152.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, (13413.91 / 49152.00)\n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 1\n",
    "n_batch = 512\n",
    "model_path = \"/Users/bsantanna/dev/workspace/community/Llama-2-7b-chat-hf/ggml-model-f16.bin\"\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    f16_kv=True,\n",
    "    verbose=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T12:12:46.053698Z",
     "start_time": "2023-08-15T12:12:45.902106Z"
    }
   },
   "id": "c518d02544844df3"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"Suggest a company name to describe a company that makes {product}\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T12:12:46.053852Z",
     "start_time": "2023-08-15T12:12:46.051249Z"
    }
   },
   "id": "3da569c987641877"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The name of the company should be catchy, unique, and memorable.\n",
      "Example: \"Coach\" (a well-known luxury handbag brand)\n",
      "\n",
      "Here are some suggestions for a company name that describes handbags that are artistic and visually stunning:\n",
      "\n",
      "1. Artifacts & Luxe - This name plays off the idea of handbags being works of art, while also highlighting the luxurious nature of the products.\n",
      "2. Canvas & Craft - This name combines the idea of a blank canvas with the craftsmanship involved in creating beautiful handbags.\n",
      "3. Elegance & Edge - This name suggests a company that creates both elegant and sophisticated designs, while also incorporating an edgy or modern twist.\n",
      "4. Handcrafted Haven - This name emphasizes the artisanal nature of the products, while also evoking a sense of safety and security in the hands of a skilled craftsman.\n",
      "5. Luxe & Loops - This name combines the idea of luxury with the repetition of loops, which could represent both the weaving of fabric and the loops of a bag's straps\n"
     ]
    }
   ],
   "source": [
    "product = \"Artistic and visually stunning hand bags.\"\n",
    "result = chain.run(product)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T12:13:04.844501Z",
     "start_time": "2023-08-15T12:12:46.054541Z"
    }
   },
   "id": "6bb9f8eb7d137e2e"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# prompt template 1: translate to english\n",
    "first_prompt = PromptTemplate.from_template(\n",
    "    \"Summarize the text enclosed by <> with your own words: <{review}>\"\n",
    ")\n",
    "\n",
    "chain_one = LLMChain(\n",
    "    llm=llm, prompt=first_prompt,\n",
    "    output_key=\"summary\"\n",
    ")\n",
    "\n",
    "second_prompt = PromptTemplate.from_template(\n",
    "    \"Determine the overall sentiment of the customer review enclosed by <> either Positive or Negative, reply using a single option: <{summary}>\"\n",
    ")\n",
    "\n",
    "chain_two = LLMChain(\n",
    "    llm=llm, prompt=second_prompt,\n",
    "    output_key=\"sentiment\"\n",
    ")\n",
    "\n",
    "third_prompt = PromptTemplate.from_template(\n",
    "    \"Extract key words from the text enclosed by <>, reply only words separated by comma: <{summary}>\"\n",
    ")\n",
    "\n",
    "chain_three = LLMChain(\n",
    "    llm=llm, prompt=third_prompt,\n",
    "    output_key=\"keywords\"\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T13:28:56.838917Z",
     "start_time": "2023-08-15T13:28:56.830725Z"
    }
   },
   "id": "3452f24c59909fe7"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review': 'Hotel was kept clean and the staff around the pools, restaurant and bar worked hard. We were disappointed with the evening entertainment as there wasn’t much. We found the food very bland particularly at dinner and ended up going out for some evening meals. We had a pool view which was nice initially but realised the first morning it meant you were woken very early by the cleaning machines around the pool. The bed dipped in the middle and our room wasn’t brushed out or mopped during the duration of our stay.', 'summary': '\\nThe hotel was well-maintained, and the staff provided good service at the pools, restaurant, and bar. However, the evening entertainment was lacking, and the food was bland, particularly at dinner. The guests were disappointed with the pool view as it meant they were woken up early by cleaning machines. Additionally, the beds were not properly maintained during their stay, and the rooms were not properly cleaned.', 'sentiment': '\\nOverall Sentiment: Negative', 'keywords': '\\nWords: good service, well-maintained, bland, disappointed'}\n",
      "\n",
      "Duration: 15.89 seconds\n"
     ]
    }
   ],
   "source": [
    "# overall_chain: input= review \n",
    "# and output= language, english_review,summary, sentiment\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three],\n",
    "    input_variables=[\"review\"],\n",
    "    output_variables=[\"summary\", \"sentiment\", \"keywords\"],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "review = \"Hotel was kept clean and the staff around the pools, restaurant and bar worked hard. We were disappointed with the evening entertainment as there wasn’t much. We found the food very bland particularly at dinner and ended up going out for some evening meals. We had a pool view which was nice initially but realised the first morning it meant you were woken very early by the cleaning machines around the pool. The bed dipped in the middle and our room wasn’t brushed out or mopped during the duration of our stay.\"\n",
    "\n",
    "start = time.time()\n",
    "result = overall_chain(review)\n",
    "end = time.time()\n",
    "duration_seconds = end - start\n",
    "print(result)\n",
    "print(f\"\\nDuration: {duration_seconds:0.2f} seconds\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T13:29:13.198775Z",
     "start_time": "2023-08-15T13:28:57.309666Z"
    }
   },
   "id": "8a7d24d6ff14d6f8"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T12:13:28.958327Z",
     "start_time": "2023-08-15T12:13:28.955956Z"
    }
   },
   "id": "7e98b4b018a2be08"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
