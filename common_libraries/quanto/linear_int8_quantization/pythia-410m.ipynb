{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Linear quantization to int8 \n",
    "\n",
    "- [Original Source: Deeplearning.ai Quantization Fundamentals Course](https://learn.deeplearning.ai/courses/quantization-fundamentals/lesson/5/quantization-theory)"
   ],
   "id": "d2f99105cb1e97d4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-23T12:43:13.882534Z",
     "start_time": "2024-04-23T12:43:13.879650Z"
    }
   },
   "source": "#!pip install transformers quanto==0.0.11 torch",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T12:43:13.909839Z",
     "start_time": "2024-04-23T12:43:13.907036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from quanto import quantize, freeze\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ],
   "id": "fc904c768800dd3b",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T12:43:13.954389Z",
     "start_time": "2024-04-23T12:43:13.946187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def named_module_tensors(module, recurse=False):\n",
    "    for named_parameter in module.named_parameters(recurse=recurse):\n",
    "      name, val = named_parameter\n",
    "      flag = True\n",
    "      if hasattr(val,\"_data\") or hasattr(val,\"_scale\"):\n",
    "        if hasattr(val,\"_data\"):\n",
    "          yield name + \"._data\", val._data\n",
    "        if hasattr(val,\"_scale\"):\n",
    "          yield name + \"._scale\", val._scale\n",
    "      else:\n",
    "        yield named_parameter\n",
    "\n",
    "    for named_buffer in module.named_buffers(recurse=recurse):\n",
    "      yield named_buffer\n",
    "\n",
    "def dtype_byte_size(dtype):\n",
    "    \"\"\"\n",
    "    Returns the size (in bytes) occupied by one parameter of type `dtype`.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    if dtype == torch.bool:\n",
    "        return 1 / 8\n",
    "    bit_search = re.search(r\"[^\\d](\\d+)$\", str(dtype))\n",
    "    if bit_search is None:\n",
    "        raise ValueError(f\"`dtype` is not a valid dtype: {dtype}.\")\n",
    "    bit_size = int(bit_search.groups()[0])\n",
    "    return bit_size // 8\n",
    "\n",
    "\n",
    "def compute_module_sizes(model):\n",
    "    \"\"\"\n",
    "    Compute the size of each submodule of a given model.\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    module_sizes = defaultdict(int)\n",
    "    for name, tensor in named_module_tensors(model, recurse=True):\n",
    "      size = tensor.numel() * dtype_byte_size(tensor.dtype)\n",
    "      name_parts = name.split(\".\")\n",
    "      for idx in range(len(name_parts) + 1):\n",
    "        module_sizes[\".\".join(name_parts[:idx])] += size\n",
    "\n",
    "    return module_sizes"
   ],
   "id": "b29e6496184602b0",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T12:43:16.835714Z",
     "start_time": "2024-04-23T12:43:13.963270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"EleutherAI/pythia-410m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True)\n",
    "module_sizes = compute_module_sizes(model)\n",
    "print(f\"The model size is {module_sizes[''] * 1e-9} GB\")\n",
    "print(model.gpt_neox)"
   ],
   "id": "5d35cb43a4d95a22",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model size is 1.6402112960000002 GB\n",
      "GPTNeoXModel(\n",
      "  (embed_in): Embedding(50304, 1024)\n",
      "  (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-23): 24 x GPTNeoXLayer(\n",
      "      (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (attention): GPTNeoXAttention(\n",
      "        (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "        (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (mlp): GPTNeoXMLP(\n",
      "        (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (act): GELUActivation()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T12:43:18.788275Z",
     "start_time": "2024-04-23T12:43:16.837974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"Today my plans are \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "fef589427b693ac2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today my plans are \n",
      "to go to the beach with my family and watch the sun set over\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T12:43:21.067757Z",
     "start_time": "2024-04-23T12:43:18.789577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "quantize(model, weights=torch.int8, activations=None)\n",
    "module_sizes = compute_module_sizes(model)\n",
    "print(f\"The model size is {module_sizes[''] * 1e-9} GB\")\n",
    "print(model.gpt_neox)\n",
    "print(model.gpt_neox.layers[0].attention.dense.weight)"
   ],
   "id": "c70b1450a757ef49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model size is 1.6402120720000002 GB\n",
      "GPTNeoXModel(\n",
      "  (embed_in): Embedding(50304, 1024)\n",
      "  (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-23): 24 x GPTNeoXLayer(\n",
      "      (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (attention): GPTNeoXAttention(\n",
      "        (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "        (query_key_value): QLinear(in_features=1024, out_features=3072, bias=True)\n",
      "        (dense): QLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (mlp): GPTNeoXMLP(\n",
      "        (dense_h_to_4h): QLinear(in_features=1024, out_features=4096, bias=True)\n",
      "        (dense_4h_to_h): QLinear(in_features=4096, out_features=1024, bias=True)\n",
      "        (act): GELUActivation()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[ 0.0061, -0.0016, -0.0068,  ..., -0.0062,  0.0138,  0.0222],\n",
      "        [ 0.0077,  0.0157, -0.0090,  ...,  0.0013, -0.0132,  0.0109],\n",
      "        [-0.0330,  0.0008,  0.0281,  ...,  0.0026,  0.0456, -0.0077],\n",
      "        ...,\n",
      "        [-0.0105,  0.0091, -0.0137,  ..., -0.0046,  0.0371, -0.0077],\n",
      "        [-0.0063,  0.0035,  0.0147,  ...,  0.0220,  0.0158,  0.0224],\n",
      "        [-0.0299,  0.0129,  0.0208,  ..., -0.0040, -0.0065,  0.0122]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T12:43:23.875240Z",
     "start_time": "2024-04-23T12:43:21.069727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "freeze(model)\n",
    "module_sizes = compute_module_sizes(model)\n",
    "print(f\"The model size is {module_sizes[''] * 1e-9} GB\")\n",
    "print(model.gpt_neox)\n",
    "print(model.gpt_neox.layers[0].attention.dense.weight)"
   ],
   "id": "b3fa6f863a371348",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model size is 0.580794472 GB\n",
      "GPTNeoXModel(\n",
      "  (embed_in): Embedding(50304, 1024)\n",
      "  (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-23): 24 x GPTNeoXLayer(\n",
      "      (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (attention): GPTNeoXAttention(\n",
      "        (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "        (query_key_value): QLinear(in_features=1024, out_features=3072, bias=True)\n",
      "        (dense): QLinear(in_features=1024, out_features=1024, bias=True)\n",
      "        (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (mlp): GPTNeoXMLP(\n",
      "        (dense_h_to_4h): QLinear(in_features=1024, out_features=4096, bias=True)\n",
      "        (dense_4h_to_h): QLinear(in_features=4096, out_features=1024, bias=True)\n",
      "        (act): GELUActivation()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "QTensor(tensor([[ 12,  -3, -14,  ..., -12,  28,  45],\n",
      "        [ 18,  37, -21,  ...,   3, -31,  26],\n",
      "        [-75,   2,  64,  ...,   6, 104, -18],\n",
      "        ...,\n",
      "        [-25,  22, -33,  ..., -11,  89, -19],\n",
      "        [-14,   8,  33,  ...,  49,  35,  50],\n",
      "        [-56,  24,  39,  ...,  -8, -12,  23]], dtype=torch.int8), scale=tensor([[0.0005],\n",
      "        [0.0004],\n",
      "        [0.0004],\n",
      "        ...,\n",
      "        [0.0004],\n",
      "        [0.0004],\n",
      "        [0.0005]]), public_dtype=torch.float32, requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T12:43:32.580097Z",
     "start_time": "2024-04-23T12:43:23.876476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"Today my plans are \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "e2109c59d3c548c2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today my plans are \n",
      "to go to the beach with my family and watch the sun set over\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T12:43:32.584082Z",
     "start_time": "2024-04-23T12:43:32.581553Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1c51c5da26333b8",
   "outputs": [],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
